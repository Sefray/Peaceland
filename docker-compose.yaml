version: "3"
services:
  peacewatcher:
    build:
      context: ./peacewatcher
    depends_on:
      - broker
    environment:
      PL_KAFKA_HOST: broker:29092
      PL_REPORT_TOPIC: reports
      PL_NB_PEACEWATCHER: 2
    deploy:
      replicas: 1

  office:
    build:
      context: ./office
    depends_on:
      - broker
    environment:
      PL_KAFKA_HOST: broker:29092
      PL_REPORT_TOPIC: reports
      PL_ALERT_TOPIC: alerts
    deploy:
      replicas: 3

  stats:
    build:
      context: ./stats
    depends_on:
      - namenode
      - spark-master
    environment:
      PL_HDFS_HOST: hdfs://namenode:9000
      PL_SPARK_MASTER: spark://spark-master:7077
      PL_REPORTS: "/topics/reports/*/*.txt"
    deploy:
      replicas: 1


  peacemaker-connector:
    build:
      context: ./peacemaker-connector
    depends_on:
      - broker
    environment:
      PL_KAFKA_HOST: broker:29092
      PL_ALERT_TOPIC: alerts
      PL_WEBHOOK_URL: https://discord.com/api/webhooks/981686381699039253/YGmN263FAzUJa7VLE5bJuXA36PULVNtiZpSeFvi92MIx_qjTwZSJhAWOzIqv4Z62bPz1
      PL_GROUP_ID: pl_group
    deploy:
      replicas: 1

  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.1
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-kafka:7.0.1
    container_name: broker
    ports:
      # To learn about configuring Kafka for access across networks see
      # https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/
      - "9092:9092"
    expose:
      - "29092"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_CREATE_TOPICS: "reports:1:3,alerts:1:3"
      TOPIC_AUTO_CREATE: false
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://broker:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

  broker-topic-setup:
    image: confluentinc/cp-kafka:7.0.1
    container_name: broker-topic-setup
    depends_on:
      - broker
    restart: "no"
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "
      echo 'Waiting for broker to be online'
      sleep 5 # prevent useless requests since the server takes time to setup

      # but make sure it is online with a simple get request
      kafka-topics --bootstrap-server broker:29092 --list

      echo 'Creating kafka topics'
      kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic reports --replication-factor 2 --partitions 2
      kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic alerts --replication-factor 2 --partitions 2

      echo 'Setup done, topics created:'
      kafka-topics --bootstrap-server broker:29092 --list
      "

  schema_registry:
    image: confluentinc/cp-schema-registry:6.2.4
    hostname: schema_registry
    container_name: schema_registry
    depends_on:
      - zookeeper
      - broker
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema_registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: "zookeeper:2181"
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_ORIGIN: "*"
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_METHODS: "GET,POST,PUT,OPTIONS"

  connect:
    image: confluentinc/cp-kafka-connect:6.2.4
    hostname: connect
    container_name: connect
    depends_on:
      - zookeeper
      - broker
    ports:
      - "8083:8083"
    expose:
      - "8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "broker:29092"
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: kafka-connect
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.storage.StringConverter
        # CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
        # CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      #  ---------------
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components,/data/confluent-hub-components
    # If you want to use the Confluent Hub installer to d/l component, but make them available
    # when running this offline, spin up the stack once and then run :
    #   docker cp kafka-connect:/usr/share/confluent-hub-components ./data/connect-jars
    volumes:
      - $PWD/kafka-connect-jars:/data
    command:
      - bash
      - -c
      - |
        echo "Installing Connector"
        # hdfs is installed via mount
        #
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run &
        #
        sleep infinity

  connectors-init:
    image: curlimages/curl:7.83.1
    container_name: connectors-init
    entrypoint: /bin/sh
    depends_on:
      - connect
    command:
      - -c
      - |
        while true; do \
          curl http://connect:8083/; \
          [ $$? -eq 0 ] && break; \
          sleep 1;
        done; \
        sleep 1; \
        echo "Activating connectors"
        curl -X POST \
        http://connect:8083/connectors \
        -H 'Content-Type: application/json' \
        -d '{
          "name": "hdfs-sink",
          "config": {
            "connector.class": "io.confluent.connect.hdfs.HdfsSinkConnector",
            "tasks.max": "1",
            "topics": "reports",
            "hdfs.url": "hdfs://namenode:9000",
            "hadoop.conf.dir": "/usr/local/hadoop/etc/hadoop",
            "hadoop.home": "/usr/local/hadoop",
            "format.class": "io.confluent.connect.hdfs.string.StringFormat",
            "flush.size": "1",
            "rotate.interval.ms": "1000"
          }
        }'; \
        echo "Connectors activated"

  ### HDFS

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
      - 9000:9000
    expose:
      - "9000"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    env_file:
      - ./hadoop.env

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      - ./hadoop.env

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env

  ### Spark

  spark-master:
    build:
      context: ./spark
    ports:
      - "9990:8080"
      - "7077:7077"
    volumes:
       - ./spark/apps:/opt/spark-apps
       - ./spark/data:/opt/spark-data
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_WORKLOAD=master

  spark-worker-1:
    build:
      context: ./spark
    ports:
      - "9991:8080"
      - "7000:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-1
    volumes:
       - ./spark/apps:/opt/spark-apps
       - ./spark/data:/opt/spark-data
  spark-worker-2:
    build:
      context: ./spark
    ports:
      - "9992:8080"
      - "7001:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-2
    volumes:
        - ./spark/apps:/opt/spark-apps
        - ./spark/data:/opt/spark-data

  # ----------------------------------------------------
  # Place holder containers made to start the other ones
  # ----------------------------------------------------

  peaceland:
    image: tianon/true
    depends_on:
      - peacewatcher
      - office
    restart: "no"

  hdfs:
    image: tianon/true
    depends_on:
      - historyserver
      - nodemanager1
      - resourcemanager
      - datanode
      - namenode
    restart: "no"

  spark:
    image: tianon/true
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
    restart: "no"

  kafka:
    image: tianon/true
    depends_on:
      - broker
      - broker-topic-setup
    restart: "no"

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:
